{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp3XPuaTu9jl"
      },
      "source": [
        "\n",
        "# How to generate text: using different decoding methods for language generation with Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GPT-2**\n",
        "\n",
        "#### ***Language Modeling***\n",
        "- ê¸°ì¡´ GPT-1ì€ pre-trainingê³¼ supervised fine-tuningì˜ ê²°í•©\n",
        "- GPT-2ëŠ” language modeling(token sequenceë¥¼ ì‚¬ìš©í•´ ë¬¸ì¥ì˜ ë¹„ì§€ë„ ë¶„í¬ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•)\n",
        "\n",
        "#### ***Various Training Dataset***\n",
        "- GPT-2ì˜ ê°€ì¥ í° ëª©ì ì€ Fine-tuning ì—†ì´ unsupervised pre-training ë§Œì„ Zero-shotìœ¼ë¡œ í†µí•´ ë‹¤ì–‘í•œ taskë¥¼ ì§„í–‰í•  ìˆ˜ ìˆëŠ” General Language Modelì„ ë§Œë“œëŠ” ê²ƒ\n",
        "- GPT-1ì€ news article, wikipediaë¥¼ ì£¼ë¡œ ì‚¬ìš©. GPT-2ëŠ” Web Text ì‚¬ìš©\n",
        "\n",
        "#### ***Byte Pair Encoding(BPE)***\n",
        "- subword ê¸°ë°˜ì˜ ì¸ì½”ë”© ë°©ë²•ìœ¼ë¡œ ë¬¸ì ë‹¨ìœ„ë¡œ ë‹¨ì–´ë¥¼ ë¶„í•´í•´ vocab ìƒì„±\n",
        "- OOV(Out Of Vocabulary) ë¬¸ì œ í•´ê²° "
      ],
      "metadata": {
        "id": "19S3UvyanLnB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fK2bQ_rS02Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxLvv6UaPa33"
      },
      "source": [
        "### **Introduction**\n",
        "\n",
        "ìµœê·¼, ëŒ€ëŸ‰ì˜ weppage ë°ì´í„°ë¡œ í•™ìŠµ ëœ transformer-based language modelsì˜ ë“±ì¥ìœ¼ë¡œ open-ended language generationì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì¡ŒìŠµë‹ˆë‹¤.(ex. GPT-2) \n",
        "\n",
        "í–¥ìƒëœ tranformer architecureì™€ ëŒ€ëŸ‰ì˜ unsupervised trainig dataì™¸ì—ë„ ë” ë‚˜ì€ decoding methods ë˜í•œ ì¤‘ìš”í•œ ì—­í• ì„ í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "ë³¸ ìë£Œì—ì„œëŠ” ë‹¤ì–‘í•œ decoding strategiesì— ëŒ€í•œ ê°„ëµí•œ ê°œìš”ì™€  `transformers` libraryë¥¼ ì‚¬ìš©í•´ ì†ì‰½ê²Œ ì´ëŸ¬í•œ decoding strategiesë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "ëª¨ë“  ë°©ë²•ë“¤ì€ **auto-regressive** language generation([here](http://jalammar.github.io/illustrated-gpt2/) a refresher)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![GPT-2 Auto regressive](http://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif)"
      ],
      "metadata": {
        "id": "0Zbm10Z96Cwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Auto-regressive* language generationëŠ” word sequenceì˜ í™•ë¥ ì€ next wordì˜ conditional distributionìœ¼ë¡œ decomposeí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ê¸°ë³¸ ê°€ì •ì…ë‹ˆë‹¤. \n",
        "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with }  w_{1: 0} = \\emptyset, $$\n",
        "\n",
        "$W_0$ëŠ” initial *context* word sequenceë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. Word sequenceì˜ ê¸¸ì´ì¸ $T$ëŠ” $P(w_{t} | w_{1: t-1}, W_{0})$ìœ¼ë¡œ ë¶€í„° EOS tokenì´ ë‚˜ì˜¨ timestep $t=T$ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "Auto-regressive language generationì€ `GPT2`, `XLNet`, `OpenAi-GPT`, `CTRL`, `TransfoXL`, `XLM`, `Bart`, `T5` in both PyTorch and Tensorflow >= 2.0!ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì´ë²ˆ ì‹¤ìŠµ ì‹œê°„ì—ëŠ” í˜„ì¬ ìœ ëª…í•œ decoding ë°©ë²•ë“¤ì¸ *Greedy search*, *Beam search*, *Top-K sampling*, *Top-p sampling* ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ToPf40Ab6J8j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si4GyYhOQMzi"
      },
      "source": [
        "Transformerë¥¼ ì„¤ì¹˜í•˜ê³  Modelì„ load í•´ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzZ_IVTtoQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f2fa81-aa2d-4a9d-a12c-cebf349d2c5b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.0.dev0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ë²ˆ ì‹¤ìŠµ ì‹œê°„ì—ì„œëŠ” SKTì—ì„œ ê³µê°œí•œ KoGPT-2 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "95OFOKD_-z8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!apt-get install git-lfs\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/taeminlee/kogpt2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L6NtIzO9awd",
        "outputId": "b11dba2d-e929-407a-fad9-40c79a12779a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected operating system as Ubuntu/bionic.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be upgraded:\n",
            "  git-lfs\n",
            "1 upgraded, 0 newly installed, 0 to remove and 56 not upgraded.\n",
            "Need to get 7,168 kB of archives.\n",
            "After this operation, 7,962 kB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu bionic/main amd64 git-lfs amd64 3.2.0 [7,168 kB]\n",
            "Fetched 7,168 kB in 1s (11.7 MB/s)\n",
            "(Reading database ... 155689 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_3.2.0_amd64.deb ...\n",
            "Unpacking git-lfs (3.2.0) over (2.3.4-1) ...\n",
            "Setting up git-lfs (3.2.0) ...\n",
            "Git LFS initialized.\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Git LFS initialized.\n",
            "Cloning into 'kogpt2'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 52 (delta 20), reused 52 (delta 20), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (52/52), done.\n",
            "Filtering content: 100% (2/2), 959.93 MiB | 56.14 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue2kOQhXTAMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547290de-141a-4e64-dd30-5db92b920796"
      },
      "source": [
        "import torch\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = SentencePieceBPETokenizer(\"/content/kogpt2/vocab.json\", \"/content/kogpt2/merges.txt\")\n",
        "\n",
        "# ì´ ëª¨ë¸ê°™ì€ ê²½ìš° vocab_sizeê°€ 50000ë¡œ ë˜ì–´ìˆìŒ.\n",
        "# ê·¸ë˜ì„œ config GPT2 loadë¥¼ í•  ë•Œ ë°˜ë“œì‹œ vocab sizeë¥¼ ë§ì¶°ì¤˜ì•¼ í•¨.\n",
        "config = GPT2Config(vocab_size=50000)\n",
        "config.pad_token_id = tokenizer.token_to_id('<pad>')\n",
        "model = GPT2LMHeadModel(config)\n",
        "\n",
        "model_dir = '/content/kogpt2/pytorch_model.bin'\n",
        "\n",
        "# map_locationì„ í•´ì¤˜ì•¼ GPUì— ì˜¬ë¼ê°€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n",
        "model.load_state_dict(torch.load(model_dir, map_location='cuda'), strict=False)\n",
        "model.to('cuda')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50000, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8Y7cgu9ohXP"
      },
      "source": [
        "### **Greedy Search**\n",
        "\n",
        "Greedy searchëŠ” ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì„ íƒí•  ë•Œ, ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ëŠ” ë‹¨ìˆœí•œ ë°©ë²•ì…ë‹ˆë‹¤.\n",
        "\n",
        "$w_t = argmax_{w}P(w | w_{1:t-1})$ at each timestep $t$. \n",
        "\n",
        "ë‹¤ìŒ ê·¸ë¦¼ì€ greedy searchë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒ ì…ë‹ˆë‹¤.\n",
        "\n",
        "![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n",
        "\n",
        "\n",
        "ì•Œê³ ë¦¬ì¦˜ì€ $\\text{\"The\"}$ì—ì„œ ì‹œì‘í•˜ì—¬ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ $\\text{\"nice\"}$ ë“±ì„ ì„ íƒí•˜ëŠ” íƒìš•ìŠ¤ëŸ¬ìš´(Greedy) ë°©ë²•ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ìµœì¢…ì ìœ¼ë¡œ ìƒì„± ëœ word sequenceëŠ” $\\text{\"The\", \"nice\", \"woman\"}$ì´ê³  ì „ì²´ í™•ë¥ ì€ $0.5 \\times 0.4 = 0.2$ ì…ë‹ˆë‹¤.\n",
        "\n",
        "`Transformers` greedy searchë¥¼ ì‚¬ìš©í•´ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWLd_J6lXz_t",
        "outputId": "bdf0cd92-6779-4134-892d-a77bf8d1af1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "def tokenizing(text):\n",
        "    return torch.tensor(tokenizer.encode(text, add_special_tokens=False).ids).unsqueeze(0).to('cuda')\n",
        "\n",
        "input_ids = tokenizing(\"ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.\")\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 100\n",
        "# ìƒì„± ëª¨ë¸ì€ generate í•¨ìˆ˜ë¥¼ í†µí•´ ë‹¤ìŒ tokenì„ ìƒì„±í•´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "# ê·¸ëƒ¥ ë„£ì–´ì£¼ë©´ ìë™ìœ¼ë¡œ greedy searchë¥¼ ì‹œì‘.\n",
        "greedy_output = model.generate(input_ids, max_length=100)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output.tolist()[0], skip_special_tokens=True))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> ì´ ê°™ì€ ì‚¬ì‹¤ì€ ì§€ë‚œ 4ì›” ë§ í•œ ì¸í„°ë„· ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œíŒì— â€˜ì´ëª¨(ì´ëª¨)ê°€ â€˜ì´ëª¨â€™ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì˜¬ë¦° ê¸€ì˜ ì¼ë¶€ë‹¤.</s><s> ì´ ê¸€ì€ â€˜ì´ëª¨â€™ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ â€˜ì´ëª¨â€™ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ â€˜ì´ëª¨â€™ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ê¸€ì„ ì˜¬ë ¸ë‹¤.</s><s> ì´ ê¸€ì—ëŠ” â€˜ì´ëª¨â€™ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ â€˜ì´ëª¨â€™ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ â€˜ì´ëª¨â€™ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ â€˜ì´ëª¨â€™ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ê¸€ì„ ì˜¬ë ¸ë‹¤.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT2ë¥¼ ì‚¬ìš©í•´ ì§§ì€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ìƒì„±ëœ ë‹¨ì–´ë“¤ì˜ ë¬¸ë§¥ì€ í•©ë¦¬ì ì´ì§€ë§Œ, ëª¨ë¸ì´ ë°˜ë³µëœ ë‹¨ì–´ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤. \n",
        "\n",
        "ì´ëŸ¬í•œ í˜„ìƒì€ ì¼ë°˜ì ì¸ ì–¸ì–´ ìƒì„± ëª¨ë¸ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ê³µí†µëœ ë¬¸ì œì¸ë°, íŠ¹íˆ Greedy Searchì™€ Beam Searchì—ì„œ ê·¸ëŸ¬í•œ í˜„ìƒì´ ë”ìš± ë‘ë“œëŸ¬ì§€ê²Œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\n",
        "\n",
        "Greedy Searchì˜ ì£¼ìš” ë‹¨ì ì€ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ë‚®ì€ í™•ë¥  ë‹¨ì–´ ì´í›„ì— ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ë” ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ë¥¼ ë†“ì¹œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. \n",
        "\n",
        "ì˜ˆë¥¼ ë“¤ì–´ ë‹¨ì–´ $\\text{has}$ëŠ” 0.9ë¡œ ë†’ì€ í™•ë¥ ì„ ê°–ì§€ë§Œ ì²«ë²ˆì§¸ ë‹¨ì–´ í›„ë³´ ì¤‘ ë‘ë²ˆì§¸ë¡œ ë†’ì€ conditional probabilityë¥¼ ê°–ëŠ” $\\text{dog}$ ì´í›„ì— ìˆ¨ì–´ ìˆëŠ” í˜•íƒœì…ë‹ˆë‹¤. ë”°ë¼ì„œ Greedy SearchëŠ” $\\text{\"The\"}, \\text{\"dog\"}, \\text{\"has\"}$ ë¼ëŠ” word sequenceë¥¼ ë†“ì¹˜ê²Œ ë©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "PN7h9WiFBJ2p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8DnXZ1WiuNd"
      },
      "source": [
        "### **Beam search**\n",
        "\n",
        "Beam searchëŠ” ê° time stepì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ hypothesisì˜ `num_beams`ë¥¼ ìœ ì§€í•˜ê³  ì „ì²´ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ hypothesisë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì¦‰, í•´ë‹¹ ì‹œì ì—ì„œ ìœ ë§í•œ ë¹”ì˜ ê°œìˆ˜ë§Œí¼(num_beams) ê³¨ë¼ì„œ ì§„í–‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë†’ì€ í™•ë¥ ì„ ê°€ì§€ê³  ìˆì§€ë§Œ ìˆ¨ì–´ìˆëŠ” word sequenceë¥¼ ë†“ì¹  ìœ„í—˜ì„ ì¤„ì…ë‹ˆë‹¤.ë‹¤ìŒ ê·¸ë¦¼ì€ `num_beams=2`ë¡œ ì„¤ì •í•œ Beam searchì˜ ì˜ˆì‹œì…ë‹ˆë‹¤:\n",
        "\n",
        "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
        "\n",
        "\n",
        "Time step $1$ : ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ hypothesisì¸ $\\text{\"The\", \"nice\"}$ ì™¸ì—ë„ beam searchëŠ” ë‘ë²ˆì§¸ë¡œ ê°€ëŠ¥ì„±ì´ ë†’ì€ $\\text{\"The\", \"dog\"}$ë¥¼ ì¶”ì í•©ë‹ˆë‹¤. \n",
        "\n",
        "Time step $2$ : beam searchëŠ” $0.2$ì˜ ê°€ëŠ¥ì„±ì„ ê°€ì§„  $\\text{\"The\", \"nice\", \"woman\"}$ë³´ë‹¤ $0.36$ì˜ ê°€ëŠ¥ì„±ì„ ê°€ì§„ $\\text{\"The\", \"dog\", \"has\"}$ê°€ í™•ë¥ ì´ ë” ë†’ë‹¤ëŠ” ê²ƒì„ ì°¾ìŠµë‹ˆë‹¤. \n",
        "\n",
        "ì´ ë°©ë²•ì€ ìš°ë¦¬ì˜ toy exampleì—ì„œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ word sequenceë¥¼ ì°¾ì•„ëƒˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "Beam searchëŠ” í•­ìƒ Greedy searchë³´ë‹¤ ë†’ì€ í™•ë¥ ì˜ ê²°ê³¼ sequenceë¥¼ ì°¾ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ì´ê²Œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²°ê³¼ë¼ê³ ëŠ” ë³´ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
        "\n",
        "`transformers`ì—ì„œ beam searchë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ ë´…ì‹œë‹¤. ìš°ë¦¬ëŠ” `num_beams > 1`, `early_stopping=True` ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  beam hypothesisê°€ eosí† í°ì— ë‹¿ìœ¼ë©´ ìƒì„±ì„ ë§ˆì¹˜ë„ë¡ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1R5kx30Ynej",
        "outputId": "8aa085ce-0f46-4c2f-857b-e7ad595bb1fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# activate beam search and early_stopping\n",
        "beam_output = model.generate(\n",
        "    input_ids,  \n",
        "    max_length=100, \n",
        "    num_beams=5, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> ê·¸ëŠ” ì´ì–´ â€œì§€ë‚œ 5ë…„ê°„ ìš°ë¦¬ ê²½ì œëŠ” ì €ì„±ì¥, ì €ì„±ì¥ì— ì§ë©´í•´ ì™”ë‹¤â€ë©° â€œì €ì„±ì¥Â·ì €ê¸ˆë¦¬Â·ê³ ë ¹í™”Â·ê³ ë ¹í™” ë“± 3ì €(ä½)ê°€ ìš°ë¦¬ ê²½ì œë¥¼ ìœ„í˜‘í•˜ëŠ” ê°€ì¥ í° ì›ì¸â€ì´ë¼ê³  ì§€ì í–ˆë‹¤.</s><s> ê·¸ëŠ” â€œì €ì„±ì¥Â·ì €ê¸ˆë¦¬Â·ê³ ë ¹í™”ë¼ëŠ” 3ì € í˜„ìƒì´ ìš°ë¦¬ ê²½ì œë¥¼ ìœ„í˜‘í•˜ëŠ” ê°€ì¥ í° ì›ì¸â€ì´ë¼ë©° â€œì €ì„±ì¥Â·ì €ê¸ˆë¦¬Â·ê³ ë ¹í™”ë¼ëŠ” 3ì € í˜„ìƒì´ ìš°ë¦¬ ê²½ì œë¥¼ ìœ„í˜‘í•˜ëŠ” ê°€ì¥ í° ì›ì¸â€ì´ë¼ê³ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê²°ê³¼ëŠ” ë” ìœ ì°½í•˜ê²Œ ë³´ì´ì§€ë§Œ ì—¬ì „íˆ ë™ì¼í•œ word sequenceë¥¼ ë°˜ë³µí•˜ëŠ” ë¬¸ì œë¥¼ í¬í•¨í•©ë‹ˆë‹¤. \n",
        "\n",
        "ë‹¨ìˆœí•œ í•´ê²°ë²•ì€ [Paulus et al. (2017)](https://arxiv.org/abs/1705.04304)ì™€ [Klein et al. (2017)](https://arxiv.org/abs/1701.02810)ì˜ ë…¼ë¬¸ì—ì„œ ì œì•ˆ ëœ n-grams í˜ë„í‹°ë¥¼ ë„ì…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê°€ì¥ ì¼ë°˜ì ì¸ n-grams í˜ë„í‹°ëŠ” ì´ë¯¸ ë‚˜íƒ€ë‚œ n-gramì— ëŒ€í•´ ë‹¤ìŒ ë‹¨ì–´ë¡œ ìƒì„± ë  í™•ë¥ ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë‘ë²ˆ ë‚˜íƒ€ë‚˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. \n",
        "\n",
        "`no_repeat_ngram_size=2`ë¥¼ ì„¤ì •í•´ 2-gramì´ ë‘ë²ˆ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì„ ë§‰ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "Yi0sRvZwGHos"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy3iVJgfnkMi",
        "outputId": "bab4d324-1a70-4c93-b885-09ef375aa06a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set no_repeat_ngram_size to 2\n",
        "beam_output = model.generate(\n",
        "    input_ids, \n",
        "    max_length=100, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> í•œí¸, ì´ë‚  ë°©ì†¡ì—ì„œëŠ” â€˜ë‚¨ìê°€ ì‚¬ë‘í•  ë•Œâ€™ì˜ ì‹ ì„¸ê²½ê³¼ ì±„ì •ì•ˆ, ì—°ìš°ì§„, ê¹€ì„±ì˜¤, ì†¡ìŠ¹í—Œ, ìœ¤ìŠ¹ì•„, ì´ì„±ë¯¼, ì±„ìƒìš°, ì´ì¬ë£¡, ìµœì§„í˜, ì˜¤ì˜ì‹¤ ë“±ì´ ì¶œì—°í•´ í™”ë ¤í•œ ì…ë‹´ì„ ê³¼ì‹œí•˜ë©° ì‹œì²­ìë“¤ì˜ ëˆˆê¸¸ì„ ëŒì—ˆë‹¤. tvN 'ê½ƒë³´ë‹¤ í• ë°°'ëŠ” í‰ê·  ì—°ë ¹ 76ì„¸ì˜ ëŒ€í•œë¯¼êµ­ ëŒ€í‘œ ì²­ì¶˜ë‚¨ë…€ê°€ ë°°ë‚­ì—¬í–‰ì˜ ë©”ì¹´ ìœ ëŸ½ìœ¼ë¡œ ë– ë‚˜ëŠ” ë¦¬ì–¼ë¦¬í‹° ì˜ˆëŠ¥ í”„ë¡œê·¸ë¨ì´ë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxsksOGDpmA0"
      },
      "source": [
        "ë”ì´ìƒ ë°˜ë³µì´ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "í•˜ì§€ë§Œ, n-gram í˜ë„í‹°ëŠ” ì‹ ì¤‘í•˜ê²Œ ì‚¬ìš©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, city New Yorkì— ëŒ€í•´ ìƒì„±ëœ ê¸°ì‚¬ëŠ” n-gramì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. 2-gramì„ ì‚¬ìš©í•˜ê²Œ ë  ê²½ìš° ì‹œì˜ ì´ë¦„ì´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í•œ ë²ˆë§Œ ë‚˜íƒ€ë‚˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "Beam searchì˜ ë˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ íŠ¹ì§•ì€ ìƒì„±ëœ Top beamì„ ë¹„êµí•˜ì—¬ ëª©ì ì— ê°€ì¥ ì í•©í•œ Beamì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "`Transformers`ì—ì„œ ìš°ë¦¬ëŠ” `num_return_sequences`ë¥¼ top-nê°œì˜ ë†’ì€ scoringì„ ê°€ì§„ beamì„ returní•  ê²ƒì¸ì§€ ì„¤ì • í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ë‹¨, num_return_sequencesëŠ” num_beamsë³´ë‹¤ ê°™ê±°ë‚˜ ì‘ì•„ì•¼í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ClO3VphqGp6",
        "outputId": "b2594c04-f3e2-4baa-c57a-07a90579ac67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set return_num_sequences > 1\n",
        "beam_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=50, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    num_return_sequences=5, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# now we have 3 output sequences\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output.tolist(), skip_special_tokens=True)))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> í•œí¸ ì´ë‚  ë°©ì†¡ì—ì„œëŠ” â€˜ì§â€™ ëª¨íƒœì†”ë¡œ íŠ¹ì§‘ì´ ì „íŒŒë¥¼ íƒ„ ê°€ìš´ë°, ë‚¨ì 1í˜¸ì™€ ì—¬ì 2í˜¸ëŠ” ì• ì •ì´Œì— ì…ì†Œí•˜ëŠ” ëª¨ìŠµì´ ì „íŒŒë¥¼ íƒ”ë‹¤. c SBS 'ì§' ë°©ì†¡ìº¡ì²˜</s>\n",
            "1: ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> í•œí¸ ì´ë‚  ë°©ì†¡ì—ì„œëŠ” â€˜ì§â€™ ëª¨íƒœì†”ë¡œ íŠ¹ì§‘ì´ ì „íŒŒë¥¼ íƒ„ ê°€ìš´ë°, ë‚¨ì 1í˜¸ì™€ ì—¬ì 2í˜¸ëŠ” ì• ì •ì´Œì— ì…ì†Œí•˜ëŠ” ëª¨ìŠµì´ ì „íŒŒë¥¼ íƒ”ë‹¤. c SBS 'ì§' ë°©ì†¡í™”ë©´ ìº¡ì²˜</s>\n",
            "2: ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> í•œí¸ ì´ë‚  ë°©ì†¡ì—ì„œëŠ” â€˜ì§â€™ ëª¨íƒœì†”ë¡œ íŠ¹ì§‘ì´ ì „íŒŒë¥¼ íƒ„ ê°€ìš´ë°, ë‚¨ì 1í˜¸ì™€ ì—¬ì 2í˜¸ëŠ” ì• ì •ì´Œì— ì…ì†Œí•˜ëŠ” ëª¨ìŠµì´ ì „íŒŒë¥¼ íƒ”ë‹¤. c SBS 'ì§' ë°©ì†¡í™”ë©´ ìº¡ì³</s>\n",
            "3: ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> í•œí¸ ì´ë‚  ë°©ì†¡ì—ì„œëŠ” â€˜ì§â€™ ëª¨íƒœì†”ë¡œ íŠ¹ì§‘ì´ ì „íŒŒë¥¼ íƒ„ ê°€ìš´ë°, ë‚¨ì 1í˜¸ì™€ ì—¬ì 2í˜¸ëŠ” ì• ì •ì´Œì— ì…ì†Œí•˜ëŠ” ëª¨ìŠµì´ ì „íŒŒë¥¼ íƒ”ë‹¤. c SBS 'ì§' ë°©ì†¡ìº¡ì²˜,\n",
            "4: ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> í•œí¸ ì´ë‚  ë°©ì†¡ì—ì„œëŠ” â€˜ì§â€™ ëª¨íƒœì†”ë¡œ íŠ¹ì§‘ì´ ì „íŒŒë¥¼ íƒ„ ê°€ìš´ë°, ë‚¨ì 1í˜¸ì™€ ì—¬ì 2í˜¸ëŠ” ì• ì •ì´Œì— ì…ì†Œí•˜ëŠ” ëª¨ìŠµì´ ì „íŒŒë¥¼ íƒ”ë‹¤. c SBS 'ì§' ë°©ì†¡ìº¡ì²˜ ë°©ì†¡\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhLKyfdbsjXc"
      },
      "source": [
        "ìœ„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ Top 5ì˜ Beam hypothesisëŠ” ì„œë¡œ ì•½ê°„ë§Œ ë‹¤ë¥¼ ë¿ì´ë©° 5ê°œë§Œ ì‚¬ìš©í–ˆì„ ê²½ìš° ë³„ë¡œ ë†€ë„ë§Œí•œ ê²°ê³¼ëŠ” ì•„ë‹™ë‹ˆë‹¤.\n",
        "\n",
        "Open-ended generationì—ì„œ, beam searchê°€ ìµœì„ ì˜ ì„ íƒì´ ì•„ë‹ ìˆ˜ ìˆëŠ” ëª‡ê°€ì§€ ì´ìœ ê°€ ì œì‹œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- Beam searchëŠ” Machine translation ë˜ëŠ” Text summarizationì²˜ëŸ¼ ì›í•˜ëŠ” ë¬¸ì¥ ìƒì„± ê¸¸ì´ê°€ ì˜ˆì¸¡ ê°€ëŠ¥í•œ taskì—ì„œëŠ” ì˜ ì‘ë™ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ Dialog ë˜ëŠ” Story generation taskì²˜ëŸ¼ ì¶œë ¥ ê¸¸ì´ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆëŠ” open-ended generationì—ì„œëŠ” ì›í™œí•˜ê²Œ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ - see [Murray et al. (2018)](https://arxiv.org/abs/1808.10006) and [Yang et al. (2018)](https://arxiv.org/abs/1808.09582). \n",
        "\n",
        "- Beam searchëŠ” ë°˜ë³µ ìƒì„± ë¬¸ì œì— ì·¨ì•½í•©ë‹ˆë‹¤. íŠ¹íˆ Story generation taskì—ì„œ n-gram ë˜ëŠ” ê¸°íƒ€ í˜ë„í‹°ë¥¼ í†µí•´ ë¬¸ì¥ì„ ì œì–´í•˜ëŠ” ê²ƒì´ ì–´ë µìŠµë‹ˆë‹¤. *ë°˜ë³µì´ ì—†ëŠ” êµ¬ë¬¸* ê³¼ *n-gram*ì˜ ë°˜ë³µ ì£¼ê¸° ì‚¬ì´ì—ì„œ ì ë‹¹í•œ trade-offë¥¼ ì°¾ê¸° ìœ„í•´ ë§ì€ fine-tuningì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "- [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751)ëŠ” ê³ í’ˆì§ˆ ì¸ê°„ ì–¸ì–´ëŠ” ë†’ì€ í™•ë¥ ì˜ ë‹¤ìŒ ë‹¨ì–´ ë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ì‰½ê²Œ ë§í•˜ìë©´ ì¸ê°„ ì…ì¥ì—ì„œ ìš°ë¦¬ëŠ” ì§€ë£¨í•˜ê±°ë‚˜ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë¬¸ì¥ì´ ì•„ë‹ˆë¼ ìš°ë¦¬ë¥¼ ë†€ë¼ê²Œ í•  ìˆ˜ ìˆëŠ” ë¬¸ì¥ ìƒì„±ì„ ì›í•œë‹¤ê³  í•©ë‹ˆë‹¤. ì €ìëŠ” ëª¨ë¸ì´ human text vs. beam seachë¥¼ graphë¡œ ë³´ì—¬ì£¼ë©´ì„œ beam search textê°€ ê·¸ë‹¤ì§€ ë†€ëì§€ ì•Šì€ ë¬¸ì¥ì„ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n",
        "\n",
        "\n",
        "So let's stop being boring and introduce some randomness ğŸ¤ª."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbbIyK84wHq6"
      },
      "source": [
        "### **Sampling**\n",
        "\n",
        "ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì˜ samplingì€ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ì–´ $w_t$ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "$$w_t \\sim P(w|w_{1:t-1})$$\n",
        "\n",
        "ì•„ë˜ ì‚¬ì§„ì€ sampling í•  ë•Œ, ì–¸ì–´ ìƒì„±ì„ ì‹œê°í™”í•œ í˜•íƒœì…ë‹ˆë‹¤.\n",
        "\n",
        "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
        "\n",
        "Samplingì„ ì´ìš©í•œ ì–¸ì–´ ìƒì„±ì€ ë”ì´ìƒ *deterministic*í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n",
        "\n",
        "ë‹¨ì–´ $\\text{\"car\"}$ ëŠ” contional probability distribution $P(w | \\text{\"The\"})$ì—ì„œ ìƒ˜í”Œë§ ë˜ê³ , $P(w | \\text{\"The\"}, \\text{\"car\"})$ëŠ” $\\text{\"drives\"}$ë¥¼ ìƒ˜í”Œë§ í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "`Transformers`ì—ì„œ ìš°ë¦¬ëŠ” `do_sample=True` ë¥¼ ì„¤ì •í•˜ê³  `top_k=0`ë¡œ ë‘ì–´ *Top-K* ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.(ë’¤ì—ì„œ ë‹¤ë£° ê²ƒ)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "3ff07fe7-1a2a-4ee6-a218-1beb5145168d",
        "id": "aRAz4D-Ks0_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# torch.manual_seed(53) #ì›í•œë‹¤ë©´ random seedë¥¼ ì§€ì • í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> 171ë…„ ìˆœì²œ ë‹¬ë™ì— ì§€ëª¨ 60 ë§Œ ì› ë²Œë˜ ì•„ë‚™ì˜ ìœ ì‚°ì€ ì²­êµ¬í–ˆë‹¤.</s><s> êµ°ì •ê³¼ì •ì€ ì•„ë¦„ë‹¤ì› ë‹¤.</s><s> ì˜›ë‚  ë…¸ì¸ë“¤ì˜ ê°ì •ì— ìˆ¨ì†Œë¦¬ë„ ì ì–´ì ¸ ë¬´ì§€ê°œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê´œì°®ì•„ ë³´ì´ì§€ë§Œ ì¼ê´€ì„±ì´ ì—†ìŠµë‹ˆë‹¤. ì´ê²ƒì€ sampling word sequencesë¥¼ í• ë•Œ ëª¨ë¸ì´ ì¼ê´€ì„±ì—†ì´ íš¡ì„¤ìˆ˜ì„¤í•˜ëŠ” ë¬¸ì¥ì„ ë°œìƒì‹œí‚¤ëŠ” í° ë¬¸ì œì…ë‹ˆë‹¤. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751)).\n",
        "\n",
        "\n",
        "í•œê°€ì§€ íŠ¸ë¦­ì€ [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max) ì˜ ì´ë¥¸ë°” `temperature`ë¥¼ ë‚®ì¶”ì–´ ë¶„í¬ $P(w|w_{1:t-1})$ë¥¼ ë” ì„ ëª…í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ì˜ ê°€ëŠ¥ì„±ì€ ì¦ê°€ì‹œí‚¤ê³  ë‚®ì€ í™•ë¥ ì˜ ë‹¨ì–´ ê°€ëŠ¥ì„±ì€ ê°ì†Œì‹œí‚¤ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. \n",
        "\n",
        "temperatureë¥¼ ì ìš©í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê·¸ë¦¼ì„ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n",
        "\n",
        "step=1ì˜ ë‹¤ìŒ ë‹¨ì–´ ë¶„í¬ëŠ” ë”ìš± ì„ ëª…í•´ì¡Œê¸° ë•Œë¬¸ì— ë‹¨ì–´ $\\text{\"car\"}$ë¥¼ ì„ íƒí•  í™•ë¥ ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "`temperature=0.7`ë¥¼ ì„¤ì •í•˜ì—¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë¶„í¬ë¥¼ ì–´ë–»ê²Œ ë³€í™”ì‹œí‚¤ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "croVNefgNC9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_k=0, \n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIBRDVLSOmpC",
        "outputId": "c9d32fe9-214f-40c7-e961-278a8d0d519c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> [ì‚¬ì§„]ê¹€ì—°ì•„,'ì¹´ë¦¬ë¸Œí•´ì»µ êµ­ì œëŒ€íšŒ'</s><s> ì´ë²ˆ ëŒ€íšŒì—ëŠ” ìŠ¤ì¦ˆí‚¤ ì•„í‚¤íˆì‚¬, ìš”ì½”íƒ€ ë©”êµ¬ë¯¸, ìŠ¤ì¦ˆí‚¤ ì•„í‚¤íˆë¡œ, í›„ë£¨í•˜ì‹œ ë‹¤ì´ìŠ¤ì¼€, í›„ì¿ í•˜ë¼ ë¯¸ë„¤ì˜¤, í›„ì¿ ì´ ë§ˆì‚¬ë¯¸, í›„ì¿ ì´ ìœ ì´ì¹˜ë¡œ, í›„ì§€íƒ€ ì•„ì•¼ì½”, ì´ì¹˜ë¡œ, ë¬´ë¼ì¹´ë¯¸ ê°€ë‚˜ì˜¤, ì˜¤ì˜¤ì‹œë§ˆ ë‚˜ì˜¤ë¯¸ ë“± 6ëª…ì˜ ì„ ìˆ˜ê°€ ì¶œì „í•œë‹¤.</s><s> ì§€ë‚œ ì‹œì¦Œ ì‹œë‹ˆì–´ ë¬´ëŒ€ì—ì„œ 5ì°¨ë¡€ ìš°ìŠ¹ì„ ì°¨ì§€í•œ '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ì œ ì´ìƒí•œ n-gramì´ ì ê³  ì¶œë ¥ ë¬¸ì¥ì´ ì¡°ê¸ˆ ë” ì¼ê´€ì„± ìˆê²Œ ìƒì„±ë©ë‹ˆë‹¤. temperatureë¥¼ ì ìš©í•˜ë©´ ë¶„í¬ê°€ ëœ randomí•˜ì§€ë§Œ `temperature` $ \\to 0$ë¡œ ì„¤ì •í•œë‹¤ë©´ temperatureê°€ ì ìš©ëœ samplingì€ greedy decodingê³¼ ê°™ì•„ì§€ë©° ì´ì „ê³¼ ë™ì¼í•œ ë¬¸ì œë¥¼ ê²ªê²Œ ë©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "TsjsAeI3Ot2w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binNTroyzQBu"
      },
      "source": [
        "### **Top-K Sampling**\n",
        "\n",
        "[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf) ì€ ê°„ë‹¨í•˜ì§€ë§Œ ë§¤ìš° ê°•ë ¥í•œ ìƒ˜í”Œë§ ë°©ì‹ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. . *Top-K* samplingì—ì„œ ë†’ì€ ê°€ëŠ¥ì„±ì„ ê°€ì§„ kê°œë¥¼ ì œì™¸í•œ ë‹¨ì–´ëŠ” í•„í„°ë§ ë˜ê³  k ì´í›„ì˜ probablity massëŠ” ì¬ë¶„ë°°ë©ë‹ˆë‹¤. GPT2ëŠ” Top-K Samplingë°©ì‹ì„ ì±„íƒí–ˆëŠ”ë°, ì´ê²ƒì´ Story Gerneration Taskì— ì„±ê³µí•œ ì´ìœ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "Top-K Samplingì„ ë” ì˜ ì„¤ëª…í•˜ê¸° ìœ„í•´ ìœ„ì˜ ì˜ˆì œì—ì„œ ë‘ Sampling stepì— ì‚¬ìš©ë˜ëŠ” ë²”ìœ„ë¥¼ 3ë‹¨ì–´ì—ì„œ 10ë‹¨ì–´ë¡œ í™•ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
        "\n",
        "\n",
        "K=6ì„ ì„¤ì •í•˜ë©´ ë‘ Sampling stepsì—ì„œ Sampling poolì„ 6ê°œì˜ ë‹¨ì–´ë¡œ ì œí•œí•©ë‹ˆë‹¤.\n",
        "\n",
        "$\\text{\"The\"}$ ë‹¤ìŒìœ¼ë¡œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” 6ê°œë¥¼ ì„ íƒí•˜ê³  $\\text{\"The:, \"car\"}$ ë’¤ì— ì˜¬ ìˆ˜ ìˆëŠ” 6ê°œë¥¼ ì„ íƒí•©ë‹ˆë‹¤. \n",
        "\n",
        "ì²« stepì—ì„œ ì „ì²´ í™•ë¥  ì§ˆëŸ‰ì˜ 2/3ì¸ 0.68ì •ë„ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ì—ì„œ ë””ì½”ë”©ë˜ì§€ë§Œ, ë‘ë²ˆì§¸ stepì—ì„œ ê±°ì˜ ëª¨ë“  í™•ë¥ ì§ˆëŸ‰ì¸ 0.99ì—ì„œ ë””ì½”ë”©í•©ë‹ˆë‹¤.\n",
        "\n",
        "ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ê·¸ê²ƒì´ ë‘ë²ˆì§¸ sampling stepì—ì„œ $\\text{\"not\", \"the\", \"small\", \"told\"}$ ì™€ ê°™ì€ ë‹¤ì†Œ ì´ìƒí•œ í›„ë³´ë“¤ì„ ì„±ê³µì ìœ¼ë¡œ ì œê±°ê°€ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "200tV_DPQCvn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBtDOdD0wx3l",
        "outputId": "f198421c-b5e0-4f0a-aa78-7d645b249940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> ë‘ í›„ë³´ ê°„ â€˜ìƒˆì •ì¹˜ê³µë™ì„ ì–¸â€™ì€ ì´ë²ˆ ì£¼ ì¤‘ìœ¼ë¡œ ì‹¤ë¬´íŒ€ì´ ë°œí‘œë  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.</s><s> ì•ì„œ ë‘ í›„ë³´ëŠ” â€˜ìƒˆì •ì¹˜ ì„ ì–¸ë¬¸ ë¶€ì†ìë£Œâ€™ë¥¼ ë¬¸ í›„ë³´ ì¸¡ì´ ì•„ì§ ë‚´ë†“ì§€ ì•Šì€ ì ì„ ë“¤ì–´\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y77H5m4ZmhEX"
      },
      "source": [
        "ì§€ê¸ˆê¹Œì§€ ë´ì˜¨ decoding methods ì¤‘ ê°€ì¥ ì‚¬ëŒë‹¤ì›Œ ë³´ì´ëŠ” í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. Top-K Samplingì˜ í•œê°€ì§€ ìš°ë ¤ë˜ëŠ” ì ì€ ë‹¤ìŒ ë‹¨ì–´ í™•ë¥  ë¶„í¬ $P(w|w_{1:t-1})$ì—ì„œ í•„í„°ë§ ëœ ë‹¨ì–´ ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì§€ ì•ŠëŠ” ì ì…ë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ ìœ„ ê·¸ë¦¼ì—ì„œ ì²«ë²ˆì§¸ stepì˜ ë‹¨ì–´ë“¤ì€ ì „ë°˜ì ìœ¼ë¡œ í‰í‰í•œ ë¶„í¬ì—ì„œ samplingë˜ì§€ë§Œ, ë‘ë²ˆì§¸ stepì˜ ì–´ë–¤ ë‹¨ì–´ë“¤ì€ ë§¤ìš° sharpí•œ ë¶„í¬ì—ì„œ sampling ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "Step $t=1$ì—ì„œ Top-Kì€ ê½¤ í•©ë¦¬ì ì¸ í›„ë³´ì²˜ëŸ¼ ë³´ì´ëŠ” $\\text{\"people\", \"big\", \"house\", \"cat\"}$ì„ ìƒ˜í”Œë§í•˜ëŠ” ê°€ëŠ¥ì„±ì„ ë°°ì œí•©ë‹ˆë‹¤. ë°˜ë©´ì— Step $t=2$ì—ì„œ ë‹¨ì–´ Sample poolì— ë‹¨ì–´ $\\text{\"down\", \"a\"}$ì™€ ê°™ì€ ë¶€ì ì ˆí•œ ë‹¨ì–´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ Sample poolì´ ê³ ì •í¬ê¸° Kë¡œ ì œí•œë˜ë©´ ëª¨í˜•ì´ Sharpí•œ ë¶„í¬ì—ì„œ íš¡ì„¤ìˆ˜ì„¤í•œ ë‹¨ì–´ë¥¼ ê³ ë¥¼ ìœ„í—˜ì´ìˆê³  í‰í‰í•œ ë¶„í¬ì—ì„œëŠ” ë¬¸ì¥ì˜ ì°½ì˜ì„±ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki9LAaexzV3H"
      },
      "source": [
        "### **Top-p (nucleus) sampling**\n",
        "\n",
        "*Top-p* samplingì€ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ Kê°œì—ì„œë§Œ sampleì„ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ëˆ„ì  í™•ë¥ ì´ í™•ë¥  pë¥¼ ì´ˆê³¼í•˜ëŠ” ìµœì†Œí•œì˜ ë‹¨ì–´ ì§‘í•©ì—ì„œ sampleì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "\n",
        "ê·¸ í›„ í™•ë¥  ì§ˆëŸ‰ì´ ë‹¨ì–´ ì§‘í•© ì‚¬ì´ì— ì¬ë¶„ë°° ë©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‹¤ìŒ ë‹¨ì–´ì˜ í™•ë¥  ë¶„í¬ì— ë”°ë¼ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ê°€ ë™ì ìœ¼ë¡œ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
        "\n",
        "$p=0.92$ë¡œ ì„¤ì •í•  ê²½ìš°, *Top-p* ëŠ” $p=92\\%$ë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ë‹¨ì–´ ìˆ˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì²«ë²ˆì§¸ ìŠ¤í…ì—ì„œ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ë‹¨ì–´ 9ê°œê°€ í¬í•¨ëœ ë°˜ë©´, ë‘ë²ˆì§¸ ìŠ¤í…ì—ì„œëŠ” top 3ê°œë§Œ ì„ íƒí•´ë„ $p=92\\%$ë¥¼ ì´ˆê³¼í•˜ê²Œ ë©ë‹ˆë‹¤. ì¦‰, ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ì—ë§Œ samplingì„ í•˜ê³  ê·¸ë ‡ì§€ ì•Šì€ ë‹¨ì–´ëŠ” samplingí•  í™•ë¥ ì´ ë§¤ìš° ì ì–´ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "`Transformers`ì—ì„œ `top_p âˆˆ (0,1)`ì„ ì„¤ì •í•˜ì—¬ *Top-p* samplingì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvwIc7YAx77F",
        "outputId": "cef854c8-071e-464a-8723-d12fce10b353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# deactivate top_k sampling and sample only from 92% most likely words\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_p=0.92, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> ë˜ ëª¨ë“  ì¥ë¹„ê°€ ì–¸ì œë“  ìš´ì˜ì´ ê°€ëŠ¥í•˜ê³  ë³´ì•ˆ ì»¨íŠ¸ë¡¤ëŸ¬ì— ê´€ì‹¬ì´ ìˆëŠ” ì‚¬ìš©ìì—ê²Œë„ í™œìš©ë„ê°€ ë†’ë‹¤.</s><s> íšŒì‚¬ê°€ í˜„ì¬ ë¯¸ë¦¬ ë³´ìœ í•˜ê³  ìˆëŠ” ëª¨ë“  ì¥ë¹„ê°€ í•´í‚¹ì— ë…¸ì¶œë˜ë©´ ê´€ë¦¬ìë‚˜ ìš´ì˜ì²´ì œ ê´€ë¦¬ìëŠ” ê°ì—¼ë¼ë„ ì›ê²©ìœ¼ë¡œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn-8gLaR4lat"
      },
      "source": [
        "ì´ë¡ ì ìœ¼ë¡œëŠ” *Top-p*ê°€ *Top-K*ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•„ ë³´ì´ì§€ë§Œ, ë‘ ë°©ë²• ëª¨ë‘ ì‹¤ì œë¡œ ì˜ ì‘ë™í•©ë‹ˆë‹¤. \n",
        "\n",
        "*Top-p*ì™€ *Top-K*ëŠ” í•¨ê»˜ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” ë§¤ìš° ë‚®ì€ ìˆœìœ„ì˜ ë‹¨ì–´ë¥¼ í”¼í•˜ë©´ì„œë„ ì¼ë¶€ ë™ì  ì„ íƒì„ í—ˆìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ë…ë¦½ì ìœ¼ë¡œ samplingëœ multiple outputsë¥¼ ì–»ê¸° ìœ„í•´ `num_return_sequences > 1`ë¡œ ì„¤ì •í•´ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kY8P9VG8Gi9",
        "outputId": "e62527c3-5ec6-4276-f55c-a8a99c4adcdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=50, \n",
        "    top_p=0.95, \n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output.tolist(), skip_special_tokens=True)))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> ë¬¸ ì˜ì›ì€ \"ë‹¹ì—°íˆ êµ­ë¯¼ì˜ ì„ íƒì´ë‹¤.</s><s> ê·¸ë ‡ë‹¤ë©´ ë¯¼ì£¼ë‹¹ì´ í•´ì•¼ í•  ì¼ì€ ë¶„ëª…íˆ ìˆë‹¤\"ë©´ì„œ \"ë¯¼ì£¼ë‹¹ì€ ë¯¼ìƒì„ ìµœìš°ì„ í•˜ê³  ë¯¼ìƒì´ íšŒë³µë˜ì§€ ì•Šìœ¼ë©´ ì•ˆ ëœë‹¤.</s><s> ë¯¼ì£¼ë‹¹ì˜ ì§„ì •í•œ ì±…ë¬´ëŠ”\n",
            "1: ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> í•˜ì§€ë§Œ ì§€ë‚œí•´ 3ì›” í•œêµ­ê°¤ëŸ½ì—ì„œ ì‹¤ì‹œí•œ ì •ë¡€ ì—¬ë¡ ì¡°ì‚¬ ê²°ê³¼ë¥¼ ë³´ë©´, ë°•ê·¼í˜œ ëŒ€í†µë ¹ì˜ ì§ë¬´ìˆ˜í–‰ì´ â€˜ì˜ í•˜ê³  ìˆë‹¤â€™ëŠ” ì‘ë‹µë¹„ìœ¨(ë³µìˆ˜ì‘ë‹µ ê°€ëŠ¥)ì€ 27.5%, â€˜ì˜ëª»í•˜ê³  ìˆë‹¤â€™\n",
            "2: ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë¶„ì•¼ë‹¤.</s><s> ë°• ì „ ëŒ€í†µë ¹ì€ ì§€ë‚œ 8ì¼ ë°• ì „ ëŒ€í†µë ¹ì˜ ìœ¡ì‚¬ í›„ë°°ê°€ ì˜ê´€ê¸‰ ì¥êµ 40ëª…ì—ê²Œ ê³¨í”„ì±„ë¡œ ê°€ê²©ë°©ì•„ ëŠ” ì‚¬ê±´ì— ì—°ë£¨ëœ ë’¤ ë‘ ë‹¬ ë™ì•ˆ ì—°ë½ì´ ëŠê¸´ ìƒíƒœë‹¤.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vRPfMl88rk0"
      },
      "source": [
        "Cool, now you should have all the tools to let your model write your stories with `transformers`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsWd7e98Vcs3"
      },
      "source": [
        "### **Conclusion**\n",
        "\n",
        "*ad-hoc* decoding methodsì— ë”°ë¥´ë©´ open-ended generationì—ì„œ *top-p* and *top-K*ëŠ” *greedy*, *beam search*ë³´ë‹¤ ë”ìš± ìœ ì°½í•œ textë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "í•˜ì§€ë§Œ  [Welleck et al. (2020)](https://arxiv.org/abs/2002.02492)ì— ë”°ë¥´ë©´ *top-K*ì™€ *top-p*ëŠ” ì—¬ì „íˆ ë°˜ë³µë˜ëŠ” word sequencesë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œ(*greedy*, *beam search*ì™€ ê°™ì´)ë¥¼ ê²ªê³  ìˆë‹¤ê³  í•©ë‹ˆë‹¤.\n",
        "\n",
        "[Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf)ëŠ” human evalutation ê´€ì ì—ì„œ, model training ëª©ì  í•¨ìˆ˜ë¥¼ ì˜ ì¡°ì •í•˜ë©´, *beam search*ê°€ *Top-p*ë³´ë‹¤ ìœ ì°½í•œ textë¥¼ ìƒì„±í•œë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "Open-ended language generationì€ ë¹ ë¥´ê²Œ ë°œì „í•˜ëŠ” ë¶„ì•¼ì´ë©°, ë¬´ì—‡ì´ ì í•©í•˜ë‹¤ê³  ë‹¨ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì—ì„œ ê°€ì¥ ì˜ ì‘ë™í•˜ëŠ” ë°©ë²•ì´ ë¬´ì—‡ì¸ì§€ ê³ ë ¤í•´ì•¼í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4CYi91h11yd"
      },
      "source": [
        "### **Appendix**\n",
        "\n",
        "There are a couple of additional parameters for the `generate` method that were not mentioned above. We will explain them here briefly!\n",
        "\n",
        "- `min_length` can be used to force the model to not produce an EOS token (= not finish the sentence) before `min_length` is reached. This is used quite frequently in summarization, but can be useful in general if the user wants to have longer outputs.\n",
        "- `repetition_penalty` can be used to penalize words that were already generated or belong to the context. It was first introduced by [Kesker et al. (2019)](https://arxiv.org/abs/1909.05858) and is also used in the training objective in [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). It can be quite effective at preventing repetitions, but seems to be very sensitive to different models and use cases, *e.g.* see this [discussion](https://github.com/huggingface/transformers/pull/2303) on Github.\n",
        "\n",
        "- `attention_mask` can be used to mask padded tokens\n",
        "- `pad_token_id`, `bos_token_id`, `eos_token_id`: If the model does not have those tokens by default, the user can manually choose other token ids to represent them.\n",
        "\n",
        "For more information please also look into the `generate` function [docstring](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate)."
      ]
    }
  ]
}